<html>

<head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/bostonprivacy.css">
</head>
	
<body>
	<div class="sidenav">
		<div class="sidenav-title">
			Boston-area Data Privacy  <br><br>
		</div>
		A web site for a Boston-area group of researchers working on data privacy.<br>
		
		<a href="index.html">Home</a>
		<a href="talks.html">Talks</a>
	</div>
	
	<div class="main">
		<div class="main-title">
			Boston-area Data Privacy
		</div>
		 <p>
		 	Below is the schedule of previous and upcoming Boston-area data privacy seminars. Join the <a href="https://groups.google.com/g/bostondataprivacy">mailing list</a> and <a href="https://calendar.google.com/calendar/u/0?cid=ZmN2MDBuNGd2MjdwazY0M2E4OXEwNDVqajRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ">Google calendar</a> for more information, including the Zoom meeting links.
		 </p>

		<div class="schedule">
			<table width="100%">
				<tr><td colspan=3><center><strong>=== Upcoming Talks ===</strong></center></td></tr>
				<tr>
					<th width="12%">Date</th>
					<th width="70%">Talk</th>
				</tr>
				

				<tr><td>Friday, March 19 at 11-12:30 ET</td>
				<td><strong>On Distributed Differential Privacy and Counting Distinct Elements</strong><br>

						
					<br> Speaker: <a href="http://www.mit.edu/~lijieche/">Lijie Chen, MIT</a>
					<br>
					<br> Abstract: We study the setup where each of n users holds an element from a discrete set, and the goal is to count the number of distinct elements across all users, under the constraint of (eps, delta)-differentially privacy: (1) In the non-interactive local setting, we prove that the (additive) error of any protocol is Omega(n) for any constant eps and for any delta inverse polynomial in n. (2) In the single-message shuffle setting, we prove a lower bound of n/polylog(n) on the error for any constant eps and for some delta inverse quasi-polynomial in n. We do so by building on the moment-matching method from the literature on distribution estimation. (3) In the multi-message shuffle setting, we give a protocol with at most one message per user in expectation and with an error of sqrt{n} polylog(n) for any constant eps and for any delta inverse polynomial in n. Our protocol is also robustly shuffle private, and our error of sqrt{n} matches a known lower bound for such protocols. Our proof technique relies on a new notion, that we call dominated protocols, and which can also be used to obtain the first non-trivial lower bounds against multi-message shuffle protocols for the well-studied problems of selection and learning parity. Our lower bound for estimating the number of distinct elements provides the first omega(sqrt{n}) separation between global sensitivity and error in local differential privacy, thus answering an open question of Vadhan (2017). We also provide a simple construction that gives n/polylog(n) separation between global sensitivity and error in two-party differential privacy, thereby answering an open question of McGregor et al. (2011). This is joint work with Badih Ghazi, Ravi Kumar, and Pasin Manurangsi from Google Research.
					</td>
				</tr>

				<tr><td colspan=3><center><strong>=== Previous Talks ===</strong></center></td></tr>

				<tr><td>Friday, March 12 at 11-12:30 ET</td>
				<td><strong>Algorithmic Challenges in Efficient Training of Private (Deep) Language Models</strong><br>

						
					<br> Speaker: <a href="https://users.cs.duke.edu/~kulkarni/">Janardhan Kulkarni, Microsoft Research</a>
					<br><a href="https://bostonu.zoom.us/rec/share/tGLTd-3Psjbo57-7YSBd_nz7dVkI65yLj5eHriVgKM-be47Mk2E9OnsEVJtRynUU.Aqu9mMpYqQlzKgCc">View recording here</a><br>

					<br> Abstract: Many attacks have shown that deep learning models trained on private data of users can leak sensitive information of the users. Differential Privacy is a provable way to prevent such attacks. However, training deep learning models using DP introduces several new challenges both in terms of privacy vs accuracy tradeoffs and in the resource cost of the process. In this talk, I will highlight some of the problems we encountered, our solutions for resolving them and mention many important open problems.
					</td>
				</tr>

				<tr><td>Friday, March 5 at 11-12:30 ET</td>
				<td><strong>Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling</strong><br>

						
					<br> Speaker: <a href="https://audramarymcmillan.wixsite.com/mysite">Audra McMillan, Apple</a>
					<br><a href="https://bostonu.zoom.us/rec/share/M3vepl3QEstFGCp3b1hO3bWvKUfA0E8euaaSRYT8u-bU-_djQ58falP6ET26M1tu.PugujkKTHYhyAoqi">View recording here</a><br>
					<br> Abstract: Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and Thakurta [EFMRTT19] demonstrates that random shuffling amplifies differential privacy guarantees of locally randomized data. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously [BEMMRLRKTS17] and has led to significant interest in the shuffle model of privacy [CSUZZ19, EFMRTT19]. In this talk, we will discuss a new result on privacy amplification by shuffling, which achieves the asymptotically optimal dependence in the local privacy parameter. Our result is based on a new proof strategy which is simpler than previous approaches, and extends to approximate differential privacy with nearly the same guarantees. We'll discuss this proof strategy, the extension to approximate differential privacy, and time permitting, some of the implications of this result.
					</td>
				</tr>
				<tr><td>Friday, February 26 at 11-12:30 ET</td>
				<td><strong>Sample-efficient proper PAC learning with approximate differential privacy</strong><br>

						
					<br> Speaker: <a href="https://noahgol.github.io/">Noah Golowich, MIT</a>
					<br><a href=" https://bostonu.zoom.us/rec/share/F4tBbxD0hAZxoDZDse0GlPKYULXQsaTRa-9ThgWtSFDAfuoS8sY2Ac6pAcwx5aAx.1HEGHa-8rmnDSDu4">View recording here</a>
					<br>
					<br> Abstract: An exciting recent development in the theory of differentially private machine learning is the connection between private learning and online learning, and in particular the result that a binary hypothesis class is privately learnable if and only if it is online learnable (i.e., has finite Littlestone dimension). In this talk I will discuss our work strengthening various aspects of the result that online learning implies private learning: first, we show that the sample complexity of properly learning a class of Littlestone dimension d with approximate differential privacy is Õ(d^6), ignoring privacy and accuracy parameters. This result answers a question of Bun et al. (2020) by improving upon their upper bound of 2^O(d) on the sample complexity. Prior to our work, finiteness of the sample complexity for privately learning a class of finite Littlestone dimension was only known for improper private learners, and the fact that our learner is proper answers another question of Bun et al., which was also asked by Bousquet et al. (2020). Using machinery developed by Bousquet et al., we also show that the sample complexity of sanitizing a binary hypothesis class is at most polynomial in its Littlestone dimension and dual Littlestone dimension. This implies that a class is sanitizable if and only if it has finite Littlestone dimension. An important ingredient of our proofs is a new property of binary hypothesis classes that we call irreducibility, which may be of independent interest.
					</td>
				</tr>

				<tr><td>Friday, February 19 at 11-12:30 ET</td>
				<td><strong>Leveraging Heuristics for Private Synthetic Data Release</strong><br>

						
					<br> Speaker: <a href="https://zstevenwu.com/">Steven Wu, CMU</a>
					<br><a href="https://bostonu.zoom.us/rec/share/ioLAf5fw6mO9pMXj7lazxsODSOz-VFmWelJuiDeZyHhgr9iOQgT1qhc9KYV8JA06.uy3mguQfCViIWo38">View recording here</a><br>
					<br>
					<br> Abstract: 
					This talk will focus on differentially private synthetic data---a privatized version of the dataset that consists of fake data records and that approximates the real dataset on important statistical properties of interest. I will present our recent results on private synthetic data that leverage practical optimization heuristics to circumvent the computational bottleneck in existing work. Our techniques are motivated by a modular, game-theoretic framework, which can flexibly work with methods such as integer program solvers and deep generative models.
					</td>
				</tr>

				<tr>
					<td>Friday, February 12 at 11-12:30 ET</td>

					<td><strong>Towards Good Statistical Inference from Differentially Private Data</strong><br>
						
					<br> Speaker: <a href="https://ruobingong.github.io/">Ruobin Gong, Rutgers University</a>

					<br> <a href="https://bostonu.zoom.us/rec/share/TpCfKrWmz7F7uKxxJousscb64FTVYPJG0BvfesOH32WtZwmGIF_SAG5fq0EVM_OP.kxB_E2Mhk8wMEBa2">View recording here</a><br>
					
					<br> Abstract: Differential privacy (DP) brings provability and transparency to statistical disclosure limitation. When data users migrate their analysis to private data, there is no guarantee that a statistical model, otherwise good for non-private data, will still produce trustworthy conclusions. This talk contemplates two challenges faced by data users to draw good statistical inference from private data releases. When the DP mechanism is transparent, I discuss how approximate computation techniques (Monte Carlo EM, approximate Bayesian computation) can be systematically adapted to produce exact inference with respect to the joint specification of the intended model and the DP mechanism. In the presence of mandated invariants which the data curator must observe, I advocate for the congenial design of the DP mechanism via standard probabilistic conditioning on the invariant margins, as an alternative to optimization-based post-processing. This proposal preserves both the  privacy guarantee of the output and its statistical intelligibility. A demonstration of restricted contingency table privatization is performed via a Markov chain algorithm.
					</td>
				</tr>
				<tr>
					<td>Friday, February 5 at 11-12:30 ET</td>

					<td><strong>Private Mean Estimation of Heavy-Tailed Distributions</strong><br>
						
					<br> Speaker: Vikrant Singhal
					<br> Abstract: We give new upper and lower bounds on the minimax sample complexity of differentially private mean estimation of distributions with bounded $k$-th moments. Roughly speaking, in the univariate case, we show that $$n = \Theta\left(\frac{1}{\alpha^2} + \frac{1}{\alpha^{\frac{k}{k-1}}\varepsilon}\right)$$ samples are necessary and sufficient to estimate the mean to $\alpha$-accuracy under $\varepsilon$-differential privacy, or any of its common relaxations. This result demonstrates a qualitatively different behavior compared to estimation absent privacy constraints, for which the sample complexity is identical for all $k \geq 2$. We also give algorithms for the multivariate setting whose sample complexity is a factor of $O(d)$ larger than the univariate case.
					</td>
				</tr>
				<tr>
					<td>
						Monday, January 25 at 3PM ET
					</td>
					<td>
					  <strong>Local Differential Privacy is Equivalent to the Contraction of Hockey-Stick Divergence</strong><br>
						
					  <br>Abstract: In this talk, we first show that the approximate local differential privacy (LDP) can be equivalently expressed in terms of the contraction coefficient of “Hockey-Stick Divergence.” This result then enables us to relate the LDP guarantees of randomized mechanisms to contraction properties of any arbitrary f-divergences. This is in fact a generalization (and improvement) of the main result in  [Duchi, Jordan and Wainwright, FOCS’13] that led to information-theoretic lower bounds for private minimax estimation problems only in the high privacy regime (i.e., epsilon<1 and delta =0). Our result allows us to drop the high-privacy assumption and obtain lower bounds for any epsilon and delta. Time permitting, I will also discuss some implications for the private Bayesian estimation problems.   This is a work in progress and based on a collaboration with Maryam Aliakbarpour (UMass) and Flavio Calmon (Harvard).
					</td>
				</tr>
			</table>
		</div>	
	</div> 

</body>

</html>
