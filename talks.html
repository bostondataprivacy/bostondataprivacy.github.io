<html>

<head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/bostonprivacy.css">
</head>
	
<body>
	<div class="sidenav">
		<div class="sidenav-title">
			Boston-area Data Privacy  <br><br>
		</div>
		A web site for a Boston-area group of researchers working on data privacy.<br>
		
		<a href="index.html">Home</a>
		<a href="talks.html">Talks</a>
	</div>
	
	<div class="main">
		<div class="main-title">
			Boston-area Data Privacy
		</div>
		 <p>
		 	Below is the schedule of previous and upcoming Boston-area data privacy seminars. Join the <a href="https://groups.google.com/g/bostondataprivacy">mailing list</a> and <a href="https://calendar.google.com/calendar/u/0?cid=ZmN2MDBuNGd2MjdwazY0M2E4OXEwNDVqajRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ">Google calendar</a> for more information, including the Zoom meeting links.  
		 </p>

		<div class="schedule">
			<table width="100%">

				<!--
				<tr><td colspan=3><center><strong>=== Upcoming Talks ===</strong></center></td></tr>
				<tr>
					<th width="12%">Date</th>
					<th width="70%">Talk</th>
				</tr>
				-->

				<tr><td colspan=3><center><strong>=== Spring 2022 ===</strong></center></td></tr>

				<tr><td>Friday, January 21 at 11-12:30 ET</td>
					<td><strong>FriendlyCore: Practical Differentially Private Aggregation</strong><br>
	
							
						<br> Speaker: <a href="https://sites.google.com/view/eliadtsfadia">Eliad Tsfadia, Tel-Aviv University</a>
						<br><a href="https://youtu.be/ZWhwgMwg8Z0">View recording here</a><br>
						<br>
						<br> Abstract:  Differentially private algorithms for common metric aggregation tasks, such as clustering or averaging, often have limited practicality due to their complexity or a large number of data points that is required for accurate results.
						We propose a simple and practical tool FriendlyCore that takes a set of elements D from an unrestricted (pseudo) metric space as input.  When D has effective diameter r, FriendlyCore returns a ``stable'' subset C that includes all elements, except possibly a few outliers, and is certified to have diameter r. FriendlyCore can be used to preprocess the input before privately aggregating it, potentially simplifying the aggregation or boosting its accuracy. Surprisingly, FriendlyCore is light-weight with no dependence on the dimension.
						We empirically demonstrate its advantages in boosting the accuracy of mean estimation and clustering problems such as k-means and k-GMM, outperforming tailored methods.
						The talk is based on the joint work with Edith Cohen, Haim Kaplan, Yishay Mansour, and Uri Stemmer.
						</td>
					</tr>
					
				<tr><td>Monday, February 14 at 11:15-12:30 ET</td>
					<td><strong>Privatizing One-shot and Continual Observation Histograms</strong><br>
		
								
						<br> Speaker: <a href="https://www2.math.upenn.edu/~ryrogers/">Ryan Rogers, LinkedIn</a>
						<br>
						<br> Abstract:  Member privacy is a priority at LinkedIn and a key consideration in product design. We aim to protect member privacy by using innovative privacy engineering techniques such as differential privacy to provide valuable insights to members and policy makers.  The data privacy team at LinkedIn has developed new algorithms that work with existing real time data analytics systems, allowing us to develop systems at scale while protecting member privacy.  These algorithms privatize histograms in both the one-shot setting, where data is fixed, and for the continual observation setting, for streaming data. We also showcase some deployments of differential privacy in LinkedIn products.
						</td>
					</tr>



				<tr><td colspan=3><center><strong>=== Fall 2021 ===</strong></center></td></tr>

				<tr><td>Friday, October 15 at 11-12:30 ET</td>
				<td><strong>Learning with user-level differential privacy</strong><br>

						
					<br> Speaker: <a href="http://theertha.info">Ananda Theertha Suresh, Google</a>
					<br><a href="https://youtu.be/SxoPTMY4zEQ">View recording here</a><br>
					<br>
					<br> Abstract:  The classical setting of differential privacy assumes each
					user contributes a single sample to the dataset and preserves privacy
					by noising the output in a way that is commensurate with the maximum
					contribution of a single example. However, in many practical
					applications such as federated learning, each user can contribute
					multiple samples. In these applications, the goal is to provide
					user-level differential privacy which protects privacy of all the
					samples of the user. In this talk, we present algorithms and
					information-theoretic lower bounds for the problems of discrete
					distribution estimation, high-dimensional mean estimation, and
					empirical risk minimization under user-level differential privacy
					constraints.
					</td>
				</tr>

				<tr><td>Friday, October 22 at 11-12:30 ET</td>
					<td><strong>Mean Estimation with User-level Privacy under Data Heterogeneity</strong><br>
	
							
						<br> Speaker: <a href="https://www.engineering.columbia.edu/faculty/rachel-cummings">Rachel Cummings, Columbia University</a>
						<br><a href="https://youtu.be/jaiAN1jwQdY">View recording here</a><br>
						<br>
						<br> Abstract:  A key challenge for data analysis in the federated setting is that user data is heterogeneous, i.e., it cannot be assumed to be sampled from the same distribution. Further, in practice, different users may possess vastly different number of samples. In this work we propose a simple model of heterogeneous user data that differs in both distribution and quantity of data, and we provide a method for estimating the population-level mean while preserving user-level differential privacy. We demonstrate asymptotic optimality of our estimator within a natural class of private estimators and also prove general lower bounds on the error achievable in our problem. In particular, while the optimal non-private estimator can be shown to be linear, we show that privacy constrains us to use a non-linear estimator.
						</td>
					</tr>

				<tr><td>Friday, October 29 at 11-12:30 ET</td>
					<td><strong>Hyperparameter Tuning with Renyi Differential Privacy</strong><br>
		
								
						<br> Speaker: <a href="http://www.thomas-steinke.net">Thomas Steinke, Google</a>
						<br><a href="https://youtu.be/TIDYKOfzBYk">View recording here</a><br>
						<br>
						<br> Abstract:  For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm's hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.
						</td>
					</tr>	
				
				<tr><td>Friday, November 5 at 11-12:30 ET</td>
					<td><strong>User-Level Differentially Private Learning via Correlated Sampling</strong><br>
			
									
						<br> Speaker: <a href="https://pasin30055.github.io">Pasin Manurangsi, Google</a>
						<br><a href="https://youtu.be/gDzFehmnfEU">View recording here</a><br>
						<br>
						<br> Abstract:  Most works in learning with differential privacy (DP) have focused on the setting where each user has a single sample. In this work, we consider the setting where each user holds $m$ samples and the privacy protection is enforced at the level of each user's data.  We show that, in this setting, we may learn with a much fewer number of users. Specifically, we show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an $(\epsilon, \delta)$-DP algorithm using only $O(\log(1/\delta)/\epsilon)$ users. For $\epsilon$-DP algorithms, we show that we can learn using only $O_{\epsilon}(d)$ users even in the local model, where $d$ is the probabilistic representation dimension. In both cases, we show a nearly-matching lower bound on the number of users required.

						A crucial component of our results is a generalization of global stability [Bun, Livni, Moran, FOCS 2020]  that allows the use of public randomness. Under this relaxed notion, we employ a correlated sampling strategy to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples.
						</td>
					</tr>
				
				<tr><td>Friday, November 12 at 11-12:30 ET</td>
					<td><strong>Inference and Privacy</strong><br>
				
										
						<br> Speaker: <a href="https://people.cs.umass.edu/~sheldon/index.html">Daniel Sheldon, University of Massachusetts Amherst</a>
						<br><a href="https://youtu.be/rdP_0IyXMcM">View recording here</a><br>
						<br>
						<br> Abstract: Differential privacy requires that an algorithm is randomized “just enough” to mask the contribution of any one individual in the input data set when viewing the algorithm’s output. This leads to new inference problems where we wish to reason about the truth given measurements with excess noise added for privacy. I will discuss recent work on inference and differential privacy, including probabilistic inference for more accurately answering database queries under privacy constraints, private Bayesian inference, and private confidence interval construction. Time permitting, I will briefly mention how underlying technical ideas were motivated by, and helped advance, inference about bird migration patterns from citizen science data.

						</td>
					</tr>

				<tr><td>Friday, November 19 at 11-12:30 ET</td>
					<td><strong>Per-Query Error and Differential Privacy</strong><br>
					
											
						<br> Speaker: <a href="http://www.cse.psu.edu/~duk17/">Daniel Kifer, Penn State University</a>
						<br><a href="https://youtu.be/wMT7H50tpws">View recording here</a><br>
						<br>
						<br> Abstract: In this talk we consider the effects of per-query error on the design of query answering algorithms under differential privacy. Per-query error is closer to stakeholder expectations than metrics that are typically considered in the literature. When differentially private systems are required to produce microdata, we show that this error metric reveals a tradeoff between query answer errors that is unrelated to privacy loss budget allocations between the queries. We also consider how to optimize the choice of linear queries to satisfy per-query error requirements.
						</td>
					</tr>

				<tr><td>Friday, December 3 at 11-12:30 ET</td>
					<td><strong>Differential Privacy and Machine Unlearning</strong><br>
					
											
						<br> Speaker: <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth, University of Pennsylvania</a>
						<br><a href="https://youtu.be/34Xrv6FZEOQ">View recording here</a><br>
						<br>
						<br> Abstract: The problem of data deletion or "machine unlearning" is to remove the influence of a data point on a trained model, with computational cost that is substantially better than the baseline solution of fully retraining the model. Whereas differential privacy asks that the same algorithm run on different (neighboring) inputs yield nearby distributions, the data deletion problem requires that two different algorithms (full retraining vs. a sequence of deletion operations) yield nearby distributions when run on the same input. So the two goals are not the same: nevertheless, techniques from differential privacy carry over in natural ways to the data deletion problem. In this talk, I'll walk through two simple vignettes that illustrate this point. The work I will discuss is from a pair of papers that are joint with Varun Gupta, Chris Jung, Seth Neel, Saeed Sharifi, and Chris Waites. 
						</td>
					</tr>
					
				<tr><td colspan=3><center><strong>=== Spring 2021 ===</strong></center></td></tr>

				<tr><td>Friday, May 14 at 11-12:30 ET</td>
				<td><strong>Near-Optimal Aggregation in the Shuffle DP Model</strong><br>

						
					<br> Speaker: <a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi, Google</a>
					<br><a href="https://bostonu.zoom.us/rec/share/dU77nqNGyfxXUUm6EGZVpwGg_bWDgMBYTr8v4gD5VicjamFAWiPX2tOK2rLEx5Ze.iVoFUX16L7vfbP0x">View recording here</a><br>
					<br> Abstract:  The shuffle model of differential privacy has recently witnessed significant interest as an intermediate setting between the well-studied central and local differential privacy models. In this talk, we describe some algorithms for differentially private aggregation in the shuffle model, achieving near-central accuracy and small communication overhead. We also present some empirical evaluations and discuss some challenges towards obtaining more scalable practical implementations. 

					Based on joint works with Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, Amer Sinha and Ameya Velingker that appeared at Eurocrypt 2020, ICML 2020 and will appear at ICML 2021.
					</td>
				</tr>

				<tr><td>Friday, May 7 at 11-12:30 ET</td>
				<td><strong>Identification and formal privacy guarantees</strong><br>

						
					<br> Speaker: <a href="https://economics.virginia.edu/people/profile/n4w/">Denis Nekipelov, University of Virginia</a>
					<br> <a href="https://bostonu.zoom.us/rec/share/EH-aU-m_8XhrSRNdUcPpWkNO7gnx_R-viV1kqrmNJ3spI2-uymOIRi4oIr9e3p7Q.-Zsedje7WDrSsMps">View recording here </a><br>
					<br> Abstract:  Empirical economic research crucially relies on highly sensitive individual datasets.
						At the same time, increasing availability of public individual-level data that comes from social
						networks, public government records and directories makes it possible for adversaries to potentially de-identify anonymized records in sensitive research datasets. Most commonly accepted
						formal definition of an individual non-disclosure guarantee is referred to as differential privacy.
						With differential privacy in place the researcher interacts with the data by issuing queries that
						evaluate the functions of the data. Differential privacy guarantee is achieved by replacing the
						actual outcome of the query with a randomized outcome with the amount of randomness determined by the sensitivity of the outcome to individual observations in the data.
						<br>
						<br>
						While differential privacy does provide formal non-disclosure guarantees, its impact on the
						identification of empirical economic models as well as its impact on the performance of estimators in nonlinear empirical Econometric models has not been sufficiently studied. Since privacy
						protection mechanisms are inherently finite-sample procedures, we define the notion of identifiability of the parameter of interest under differential privacy as a property of the limit of
						experiments. It is naturally characterized by the concepts from the random sets theory and is
						linked to the asymptotic behavior in measure of differentially private estimators.
						We demonstrate that particular instances of regression discontinuity design and average
						treatment effect may be problematic for inference with differential privacy. Those parameters
						turn out to be neither point nor partially identified. The set of differentially private estimators
						converges weakly to a random set. This result is clearly supported by our simulation evidence.
						Our analysis suggests that many other estimators that rely on nuisance parameters may have
						similar properties with the requirement of differential privacy. Identification becomes possible
						if the target parameter can be deterministically localized within the random set. In that case,
						a full exploration of the random set of the weak limits of differentially private estimators can
						allow the data curator to select a sequence of instances of differentially private estimators that is
						guaranteed to converge to the target parameter in probability. We provide a decision- theoretic
						approach to this selection.
					</td>
				</tr>

				<tr><td>Friday, April 30 at 11-12:30 ET</td>
				<td><strong>Private Stochastic Convex Optimization</strong><br>

						
					<br> Speaker: <a href="http://kunaltalwar.org/">Kunal Talwar, Apple</a>
					<br> <a href="slides/KunalTalwar.pdf">View slides here </a><br>
					<br> Abstract:  I will summarize some recent works on differentially private (DP) algorithms for stochastic convex optimization: the problem of minimizing the population loss given i.i.d. samples from a distribution over convex loss functions. In the standard l2/l2 setting, we will see two approaches to getting optimal rates for this problem. We show that for a wide range of parameters, privacy causes no additional overhead in accuracy or run time. In the process, we will develop techniques for private stochastic optimization that work for other geometries. For the LASSO setting when optimizing over the l1 ball, we will see private algorithms that achieve optimal rates. Based on joint works with various subsets of Hilal Asi, Raef Bassily, Vitaly Feldman, Tomer Koren and Abhradeep Thakurta. 
					</td>
				</tr>

				<tr><td>Friday, April 23 at 11-12:30 ET</td>
				<td><strong>Privacy is Not an Afterthought</strong><br>

						
					<br> Speaker: <a href="https://cs.uwaterloo.ca/~xihe/">Xi He, University of Waterloo</a>
					<br> <a href="https://bostonu.zoom.us/rec/share/c4XdNs4kJFuCNU3ua4rAwjvBI-nF2p-qjQsQyNkv8haNVze5azItqGVjMmrmjk0H.7KomFm957g-pX_cM">View recording here </a><br>
					<br> Abstract:  Computing technology has enabled massive digital traces of our personal lives to be collected and stored. These datasets play an important role in numerous real-life applications and research analysis, such as contact tracing for COVID 19, but they contain sensitive information about individuals. When managing these datasets, privacy is usually addressed as an afterthought, engineered on top of a database system optimized for performance and usability. This has led to a plethora of unexpected privacy attacks in the news. Specialized privacy-preserving solutions usually require a group of privacy experts and they are not directly transferable to other domains. There is an urgent need for a general trustworthy database system that offers end-to-end privacy guarantees. This talk will present the challenges in designing such a system and highlight our efforts to make the system efficient, robust, and usable for database clients while achieving provable privacy guarantees.  
					</td>
				</tr>

				<tr><td>Friday, April 16 at 11-12:30 ET</td>
				<td><strong>What is (and isn't) private learning?</strong><br>
				

						
					<br> Speaker: <a href="https://floriantramer.com/">Florian Tramèr, Stanford University</a>
					<br><a href="https://bostonu.zoom.us/rec/share/OPLexRm1UxDU4lPmlNkLWW94WJEuVVM2_1LcyIgoVP289LJ_dDSCxEMwdn1jFsd0.39544P4lx5B859hQ">View recording here</a><br>
					<br> Abstract:  In the first part of this talk, I'll motivate the need for training machine learning models with formal differential privacy guarantees, by drawing on recent examples of privacy failures. In the second part, I'll demonstrate that differentially private machine learning still has a long way to go to become practical. For many canonical vision tasks, I'll show that "old-school" computer vision algorithms with handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.
					</td>
				</tr>

			

				<tr><td>Friday, April 9 at 11-12:30 ET</td>
				<td><strong>Hypothesis Selection with Privacy</strong><br>

						
					<br> Speaker: <a href="http://www.gautamkamath.com/">Gautam Kamath, University of Waterloo</a>
					<br><a href="https://bostonu.zoom.us/rec/share/f9rRed3G5azHv09zpnykgG5IJlKJycwxw2r_E85niqTD-C4ww4hUfSeFU9UHaGY1.2aON7MAJAZYR8LrH">View recording here</a><br>
					<br> Abstract: The Scheffe estimator is a classic and celebrated statistical tool, which provides a sample-efficient method for selecting the distribution from a set of hypotheses which best matches a dataset. It can be extended to the private setting, enabling near-optimal cover-based upper bounds, which tightly complement packing-based lower bounds. I will discuss applications of this method to distribution estimation and beyond, in both the central and local setting.
					Based on several related works, with Ishaq Aden-Ali, Hassan Ashtiani, Mark Bun, Sivakanth Gopi, Janardhan Kulkarni, Aleksandar Nikolov, Vikrant Singhal, Thomas Steinke, Jonathan Ullman, Zhiwei Steven Wu, and Huanyu Zhang. Arxiv links: https://arxiv.org/abs/1905.13229, https://arxiv.org/abs/2002.09465, https://arxiv.org/abs/2002.09464, https://arxiv.org/abs/2010.09929
					</td>
				</tr>

				<tr><td>Friday, April 2 at 11-12:30 ET</td>
				<td><strong>What Is The Sample Complexity of Differentially Private Learning?</strong><br>

						
					<br> Speaker: <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran, Technion</a>
					<br><a href="https://bostonu.zoom.us/rec/share/CdEmtdI-oBp_2cR7CkWiQvxhn6bjRM4xKdluUu2zzfjf1g1u9ZIe2KyNJiYTZKWz.M3e-7zSOVbm1Px58">View recording here</a><br>
					<br> Abstract: The increase in machine learning applications which involve private and personal data highlights the need for algorithms that handle the data *responsibly*. 
					While this need has been successfully addressed by the field of differentially private machine learning, the cost of privacy remains poorly understood: 

					How much data is needed for differentially private learning? 

					How much more data does private learning require compared to learning without privacy constraints?

					We will survey some of the recent progress towards answering these questions in the distribution-free PAC model, including the Littlestone-dimension-based *qualitative* characterization and the relationship with online learning. 
					If time allows, we will also discuss this question in more general (distribution- and data-dependent) learning models.
					</td>
				</tr>

				<tr><td>Friday, March 26 at 11-12:30 ET</td>
				<td><strong>Security and Privacy Guarantees in Machine Learning with Differential Privacy</strong><br>

						
					<br> Speaker: <a href="https://roxanageambasu.github.io/">Roxana Geambasu, Columbia University</a>
					<br><a href="https://bostonu.zoom.us/rec/share/9E_gHpsay3W3WWK-ralwwmn1DxhIpUmCUC5qR8GCsNq3sm071I7sxCItGBzjvQvK.u7lp5dg6t2EgkH9L">View recording here</a><br>
					<br> Abstract: Machine learning (ML) is driving many of our applications and life-changing decisions.  Yet, it is often brittle and unstable, making decisions that are hard to understand or can be exploited.  Tiny changes to an input can cause dramatic changes in predictions; this results in decisions that surprise, appear unfair, or enable attack vectors such as adversarial examples.  Moreover, models trained on users' data can encode not only general trends from large datasets but also very specific, personal information from these datasets; this threatens to expose users' secrets through ML models or predictions.  This talk positions differential privacy (DP) -- a rigorous privacy theory -- as a versatile foundation for building into ML much-needed guarantees of security, stability, and privacy.  I first present PixelDP (S&P'19), a scalable certified defense against adversarial example attacks that leverages DP theory to guarantee a level of robustness against these attacks.  I then present Sage (SOSP'19), a DP ML platform that bounds the cumulative leakage of secrets through models while addressing some of the most pressing challenges of DP, such as running out of privacy budget and the privacy-accuracy tradeoff.  PixelDP and Sage are designed from a pragmatic, systems perspective and illustrate that DP theory is powerful but requires adaptation to achieve practical guarantees for ML workloads.
					</td>
				</tr>


				<tr><td>Friday, March 19 at 11-12:30 ET</td>
				<td><strong>On Distributed Differential Privacy and Counting Distinct Elements</strong><br>

						
					<br> Speaker: <a href="http://www.mit.edu/~lijieche/">Lijie Chen, MIT</a>
					<br><a href="https://bostonu.zoom.us/rec/share/n_7OapoERy6eKNDisJ_hu-2tbonYteVM_g6pk5vygYCjI9U_5elBcRcmZKpGveMK.ber9t0i4iAbLWWi1">View recording here</a>
					<br>
					<br> Abstract: We study the setup where each of n users holds an element from a discrete set, and the goal is to count the number of distinct elements across all users, under the constraint of (eps, delta)-differentially privacy: (1) In the non-interactive local setting, we prove that the (additive) error of any protocol is Omega(n) for any constant eps and for any delta inverse polynomial in n. (2) In the single-message shuffle setting, we prove a lower bound of n/polylog(n) on the error for any constant eps and for some delta inverse quasi-polynomial in n. We do so by building on the moment-matching method from the literature on distribution estimation. (3) In the multi-message shuffle setting, we give a protocol with at most one message per user in expectation and with an error of sqrt{n} polylog(n) for any constant eps and for any delta inverse polynomial in n. Our protocol is also robustly shuffle private, and our error of sqrt{n} matches a known lower bound for such protocols. Our proof technique relies on a new notion, that we call dominated protocols, and which can also be used to obtain the first non-trivial lower bounds against multi-message shuffle protocols for the well-studied problems of selection and learning parity. Our lower bound for estimating the number of distinct elements provides the first omega(sqrt{n}) separation between global sensitivity and error in local differential privacy, thus answering an open question of Vadhan (2017). We also provide a simple construction that gives n/polylog(n) separation between global sensitivity and error in two-party differential privacy, thereby answering an open question of McGregor et al. (2011). This is joint work with Badih Ghazi, Ravi Kumar, and Pasin Manurangsi from Google Research.
					</td>
				</tr>


				<tr><td>Friday, March 12 at 11-12:30 ET</td>
				<td><strong>Algorithmic Challenges in Efficient Training of Private (Deep) Language Models</strong><br>

						
					<br> Speaker: <a href="https://users.cs.duke.edu/~kulkarni/">Janardhan Kulkarni, Microsoft Research</a>
					<br><a href="https://bostonu.zoom.us/rec/share/tGLTd-3Psjbo57-7YSBd_nz7dVkI65yLj5eHriVgKM-be47Mk2E9OnsEVJtRynUU.Aqu9mMpYqQlzKgCc">View recording here</a><br>

					<br> Abstract: Many attacks have shown that deep learning models trained on private data of users can leak sensitive information of the users. Differential Privacy is a provable way to prevent such attacks. However, training deep learning models using DP introduces several new challenges both in terms of privacy vs accuracy tradeoffs and in the resource cost of the process. In this talk, I will highlight some of the problems we encountered, our solutions for resolving them and mention many important open problems.
					</td>
				</tr>

				<tr><td>Friday, March 5 at 11-12:30 ET</td>
				<td><strong>Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling</strong><br>

						
					<br> Speaker: <a href="https://audramarymcmillan.wixsite.com/mysite">Audra McMillan, Apple</a>
					<br><a href="https://bostonu.zoom.us/rec/share/M3vepl3QEstFGCp3b1hO3bWvKUfA0E8euaaSRYT8u-bU-_djQ58falP6ET26M1tu.PugujkKTHYhyAoqi">View recording here</a><br>
					<br> Abstract: Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and Thakurta [EFMRTT19] demonstrates that random shuffling amplifies differential privacy guarantees of locally randomized data. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously [BEMMRLRKTS17] and has led to significant interest in the shuffle model of privacy [CSUZZ19, EFMRTT19]. In this talk, we will discuss a new result on privacy amplification by shuffling, which achieves the asymptotically optimal dependence in the local privacy parameter. Our result is based on a new proof strategy which is simpler than previous approaches, and extends to approximate differential privacy with nearly the same guarantees. We'll discuss this proof strategy, the extension to approximate differential privacy, and time permitting, some of the implications of this result.
					</td>
				</tr>
				<tr><td>Friday, February 26 at 11-12:30 ET</td>
				<td><strong>Sample-efficient proper PAC learning with approximate differential privacy</strong><br>

						
					<br> Speaker: <a href="https://noahgol.github.io/">Noah Golowich, MIT</a>
					<br><a href=" https://bostonu.zoom.us/rec/share/F4tBbxD0hAZxoDZDse0GlPKYULXQsaTRa-9ThgWtSFDAfuoS8sY2Ac6pAcwx5aAx.1HEGHa-8rmnDSDu4">View recording here</a>
					<br>
					<br> Abstract: An exciting recent development in the theory of differentially private machine learning is the connection between private learning and online learning, and in particular the result that a binary hypothesis class is privately learnable if and only if it is online learnable (i.e., has finite Littlestone dimension). In this talk I will discuss our work strengthening various aspects of the result that online learning implies private learning: first, we show that the sample complexity of properly learning a class of Littlestone dimension d with approximate differential privacy is Õ(d^6), ignoring privacy and accuracy parameters. This result answers a question of Bun et al. (2020) by improving upon their upper bound of 2^O(d) on the sample complexity. Prior to our work, finiteness of the sample complexity for privately learning a class of finite Littlestone dimension was only known for improper private learners, and the fact that our learner is proper answers another question of Bun et al., which was also asked by Bousquet et al. (2020). Using machinery developed by Bousquet et al., we also show that the sample complexity of sanitizing a binary hypothesis class is at most polynomial in its Littlestone dimension and dual Littlestone dimension. This implies that a class is sanitizable if and only if it has finite Littlestone dimension. An important ingredient of our proofs is a new property of binary hypothesis classes that we call irreducibility, which may be of independent interest.
					</td>
				</tr>

				<tr><td>Friday, February 19 at 11-12:30 ET</td>
				<td><strong>Leveraging Heuristics for Private Synthetic Data Release</strong><br>

						
					<br> Speaker: <a href="https://zstevenwu.com/">Steven Wu, CMU</a>
					<br><a href="https://bostonu.zoom.us/rec/share/ioLAf5fw6mO9pMXj7lazxsODSOz-VFmWelJuiDeZyHhgr9iOQgT1qhc9KYV8JA06.uy3mguQfCViIWo38">View recording here</a><br>
					<br>
					<br> Abstract: 
					This talk will focus on differentially private synthetic data---a privatized version of the dataset that consists of fake data records and that approximates the real dataset on important statistical properties of interest. I will present our recent results on private synthetic data that leverage practical optimization heuristics to circumvent the computational bottleneck in existing work. Our techniques are motivated by a modular, game-theoretic framework, which can flexibly work with methods such as integer program solvers and deep generative models.
					</td>
				</tr>

				<tr>
					<td>Friday, February 12 at 11-12:30 ET</td>

					<td><strong>Towards Good Statistical Inference from Differentially Private Data</strong><br>
						
					<br> Speaker: <a href="https://ruobingong.github.io/">Ruobin Gong, Rutgers University</a>

					<br> <a href="https://bostonu.zoom.us/rec/share/TpCfKrWmz7F7uKxxJousscb64FTVYPJG0BvfesOH32WtZwmGIF_SAG5fq0EVM_OP.kxB_E2Mhk8wMEBa2">View recording here</a><br>
					
					<br> Abstract: Differential privacy (DP) brings provability and transparency to statistical disclosure limitation. When data users migrate their analysis to private data, there is no guarantee that a statistical model, otherwise good for non-private data, will still produce trustworthy conclusions. This talk contemplates two challenges faced by data users to draw good statistical inference from private data releases. When the DP mechanism is transparent, I discuss how approximate computation techniques (Monte Carlo EM, approximate Bayesian computation) can be systematically adapted to produce exact inference with respect to the joint specification of the intended model and the DP mechanism. In the presence of mandated invariants which the data curator must observe, I advocate for the congenial design of the DP mechanism via standard probabilistic conditioning on the invariant margins, as an alternative to optimization-based post-processing. This proposal preserves both the  privacy guarantee of the output and its statistical intelligibility. A demonstration of restricted contingency table privatization is performed via a Markov chain algorithm.
					</td>
				</tr>
				<tr>
					<td>Friday, February 5 at 11-12:30 ET</td>

					<td><strong>Private Mean Estimation of Heavy-Tailed Distributions</strong><br>
						
					<br> Speaker: Vikrant Singhal
					<br> Abstract: We give new upper and lower bounds on the minimax sample complexity of differentially private mean estimation of distributions with bounded $k$-th moments. Roughly speaking, in the univariate case, we show that $$n = \Theta\left(\frac{1}{\alpha^2} + \frac{1}{\alpha^{\frac{k}{k-1}}\varepsilon}\right)$$ samples are necessary and sufficient to estimate the mean to $\alpha$-accuracy under $\varepsilon$-differential privacy, or any of its common relaxations. This result demonstrates a qualitatively different behavior compared to estimation absent privacy constraints, for which the sample complexity is identical for all $k \geq 2$. We also give algorithms for the multivariate setting whose sample complexity is a factor of $O(d)$ larger than the univariate case.
					</td>
				</tr>
				<tr>
					<td>
						Monday, January 25 at 3PM ET
					</td>
					<td>
					  <strong>Local Differential Privacy is Equivalent to the Contraction of Hockey-Stick Divergence</strong><br>
						
					  <br>Abstract: In this talk, we first show that the approximate local differential privacy (LDP) can be equivalently expressed in terms of the contraction coefficient of “Hockey-Stick Divergence.” This result then enables us to relate the LDP guarantees of randomized mechanisms to contraction properties of any arbitrary f-divergences. This is in fact a generalization (and improvement) of the main result in  [Duchi, Jordan and Wainwright, FOCS’13] that led to information-theoretic lower bounds for private minimax estimation problems only in the high privacy regime (i.e., epsilon<1 and delta =0). Our result allows us to drop the high-privacy assumption and obtain lower bounds for any epsilon and delta. Time permitting, I will also discuss some implications for the private Bayesian estimation problems.   This is a work in progress and based on a collaboration with Maryam Aliakbarpour (UMass) and Flavio Calmon (Harvard).
					</td>
				</tr>
			</table>
		</div>	
	</div> 

</body>

</html>
