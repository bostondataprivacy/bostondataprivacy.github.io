<html>

<head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/bostonprivacy.css">
</head>
	
<body>
	<div class="sidenav">
		<div class="sidenav-title">
			Boston-area Data Privacy  <br><br>
		</div>
		A web site for a Boston-area group of researchers working on data privacy.<br>
		
		<a href="index.html">Home</a>
		<a href="talks.html">Talks</a>
	</div>
	
	<div class="main">
		<div class="main-title">
			Boston-area Data Privacy
		</div>
		
		<div class="schedule">
			<table width="100%">
				<tr><td colspan=3><center><strong>=== Upcoming Talks ===</strong></center></td></tr>
				<tr>
					<th width="12%">Date</th>
					<th width="70%">Talk</th>
				</tr>
				<tr>
					<td>Friday, February 5 at 11-12:30 ET</td>

					<td><strong>Private Mean Estimation of Heavy-Tailed Distributions</strong><br>
						
					<br> Speaker: Vikrant Singhal
					<br> Abstract: We give new upper and lower bounds on the minimax sample complexity of differentially private mean estimation of distributions with bounded $k$-th moments. Roughly speaking, in the univariate case, we show that $$n = \Theta\left(\frac{1}{\alpha^2} + \frac{1}{\alpha^{\frac{k}{k-1}}\varepsilon}\right)$$ samples are necessary and sufficient to estimate the mean to $\alpha$-accuracy under $\varepsilon$-differential privacy, or any of its common relaxations. This result demonstrates a qualitatively different behavior compared to estimation absent privacy constraints, for which the sample complexity is identical for all $k \geq 2$. We also give algorithms for the multivariate setting whose sample complexity is a factor of $O(d)$ larger than the univariate case.
					</td>
				</tr>

				<tr><td colspan=3><center><strong>=== Past Talks ===</strong></center></td></tr>
				<tr>
					<td>
						Monday, January 25 at 3PM ET
					</td>
					<td>
					  <strong>Local Differential Privacy is Equivalent to the Contraction of Hockey-Stick Divergence</strong><br>
						
					  <br>Abstract: In this talk, we first show that the approximate local differential privacy (LDP) can be equivalently expressed in terms of the contraction coefficient of “Hockey-Stick Divergence.” This result then enables us to relate the LDP guarantees of randomized mechanisms to contraction properties of any arbitrary f-divergences. This is in fact a generalization (and improvement) of the main result in  [Duchi, Jordan and Wainwright, FOCS’13] that led to information-theoretic lower bounds for private minimax estimation problems only in the high privacy regime (i.e., epsilon<1 and delta =0). Our result allows us to drop the high-privacy assumption and obtain lower bounds for any epsilon and delta. Time permitting, I will also discuss some implications for the private Bayesian estimation problems.   This is a work in progress and based on a collaboration with Maryam Aliakbarpour (UMass) and Flavio Calmon (Harvard).
					</td>
				</tr>
			</table>
		</div>	
	</div> 

</body>

</html>
