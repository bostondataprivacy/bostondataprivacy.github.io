<html>

<head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/bostonprivacy.css">
</head>
	
<body>
	<div class="sidenav">
		<div class="sidenav-title">
			Boston-Area Data Privacy <br><br>
		</div>
		A web site for a Boston-area group of researchers working on data privacy.<br>
		
		<a href="index.html">Home</a>
		<a href="talks.html">Talks</a>
		<a href="privacy-day.html">Privacy Day</a>
		<a href="summerschool">Summer School</a>

	</div>
	
	<div class="main">
		<div class="main-title">
			Boston-Area Data Privacy
		</div>
	
	
		<h2>Charles River Privacy Day &mdash; May 11th, 2023</h2>
		
		Please join us for a day of talks on the theory and practice of statistical data privacy! 
		Inspired by the success of the <a href="https://bostoncryptoday.wordpress.com/">Charles River Crypto Days</a>, the Charles River  
		Privacy Days will resume&mdash;after a decade-long <a href="https://www.bu.edu/cs/charles-river-privacy-day/">hiatus</a>&mdash;on 
		Thursday, May 11, 2023. The event will be followed on Friday, May 12 by a <a href="https://bostoncryptoday.wordpress.com/">Crypto Day workshop</a>. 
				
		<h4>Confirmed Speakers</h4>
			<ul>
				<li><a href="https://people.eecs.berkeley.edu/~minilek/">Jelani Nelson</a>, UC Berkeley</li>
				<li><a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a>, University of Washington</li>
				<li><a href="https://scholar.harvard.edu/jsarathy/home">Jayshree Sarathy</a>, Harvard University</li>
				<li><a href="https://jess-sorrell.github.io/">Jessica Sorrell</a>, University of Pennsylvania</li>
			</ul>
		Charles River Privacy Day is organized by the <a href="bostondataprivacy.github.io">Boston-Area Data Privacy Group</a>, which spans BU, Northeastern and Harvard. 
			
		<P><strong>Registration: </strong> Attendance is free, but please register via this <a href="https://forms.gle/B66gauNeTtttzrL4A">Registration Form</a> if you plan to come.
			
		<P><strong>Location:</strong> <br>
		Center for Computational and Data Sciences Boston Univeristy, 17th floor<br>
		665 Commonwealth Ave, Boston, MA 02215<BR>
		(<a href="https://goo.gl/maps/zMV4cTpMQRqd1f8t6">map</a>)

		<P><strong>Schedule:</strong> 
		
		<div class = "schedule">
		<table class="schedule">
			<tr>
				<td> <strong>9:00am - 9:30am</strong></td>
				<td> Welcome, coffee</td>
			</tr>
			<tr>
				<td> <strong>9:30am - 10:30am</strong></td>
				<td> Sewoong Oh (UW)</br><em>Rethinking Auditing Privacy from First Principles</em></td>
			</tr>
			<tr>
				<td> <strong>10:30am - 11:00am</strong></td>
				<td> Break </td>
			</tr>
			<tr>
				<td> <strong>11:00am - 12:00pm</strong></td>
				<td> Jessica Sorrell (Penn)</br><em>Connections Between Replicability, Privacy, and Perfect Generalization</em></td>
			</tr>
			<tr>
				<td> <strong>12:00pm - 2:00pm</strong></td>
				<td> Lunch (provided)</td>
			</tr>
			<tr>
				<td> <strong>2:00pm - 3:00pm</strong></td>
				<td> Jelani Nelson (Berkeley)</br><em>TBD</em></td>
			</tr>
			<tr>
				<td> <strong>3:00pm - 3:30pm</strong></td>
				<td> Break</td>
			</tr>
			<tr>
				<td> <strong>3:30pm - 4:30pm</strong></td>
				<td> Jayshree Sarathy (Harvard)</br><em>TBD</em></td>
			</tr>
		</table>
		</div>
			
		<P><strong>Talk Information:</strong>
		<div class = "talk-info">
			<ul>
				<li><strong>Speaker:</strong> Jelani Nelson, UC Berkeley </br><strong>Title:</strong> TBD </br> <strong>Abstract:</strong> TBD</li>
				
				<li><strong>Speaker:</strong> Sewoong Oh, University of Washington </br><strong>Title:</strong> Rethinking auditing privacy from first principles </br> <strong>Abstract:</strong> Differentially private training of models is error prone due to miscalculations of the sensitivity or mistakes in the implementations. Techniques for detecting such false claims of differential privacy are critical in ensuring a trustworthy ecosystem where codes and algorithms commonly are shared. A major bottleneck in the standard statistical approaches for auditing private training is in its sample complexity. Drawing a single sample for auditing can be as computationally intense as training a model from scratch. We are in a dire need of sample efficient approaches to provide a tight lower bound on the privacy leakage. However, if we hang on to the standard definition of differential privacy, then we are fundamentally limited by the sample dependence of the Bernoulli confidence intervals involved. To break this barrier, it requires rethinking differential privacy from first principles. To this end, we introduce Lifted Differential Privacy that, while equivalent to differential privacy, allows the privacy auditor to search over a larger space of counterexamples. We exploit this lifted search space in our novel design of audits that inject multiple canary examples. By generating those canaries randomly, the Lifted DP condition allows us to reuse each trained model and run multiple binary hypothesis tests for the presence or absence of each canary. Together with a novel confidence interval that exploits the (lack of) correlation between those test statistics, we showcase the significant gain in sample complexity both theoretically and empirically.</li>
				
				<li><strong>Speaker:</strong> Jayshree Sarathy, Harvard University </br><strong>Title:</strong> TBD </br> <strong>Abstract:</strong> TBD</li>				
				
				<li><strong>Speaker:</strong> Jessica Sorrell, University of Pennsylvania </br><strong>Title:</strong> Connections Between Replicability, Privacy, and Perfect Generalization </br> <strong>Abstract:</strong> Replicability is vital to ensuring scientific conclusions are reliable, but failures of replicability have been a major issue in nearly all scientific areas of study in recent decades. A key issue underlying the replicability crisis is the explosion of methods for data generation, screening, testing, and analysis, where, crucially, only the combinations producing the most significant results are reported. Such practices (also known as p-hacking, data dredging, and researcher degrees of freedom) can lead to erroneous findings that appear to be significant, but that donâ€™t hold up when other researchers attempt to replicate them.
				In this talk, we will explore connections between replicability and other stability notions that have proven useful in ensuring statistical validity. We will discuss statistical equivalences between replicability, approximate differential privacy, and perfect generalization, as well as computational separations.

				This talk is based on work with Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, and Satchit Sivakumar.</li>

			</ul>

	</div> 

</body>

</html>
